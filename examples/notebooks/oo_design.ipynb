{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hillmaker - OO design ideas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall application design goals and objectives\n",
    "\n",
    "- should be easy to run a scenario and get all the standard outputs\n",
    "- scenario specific settings should be persistable as something like a json file\n",
    "- should be possible to generate only outputs wanted\n",
    "- should have a CLI\n",
    "- should be importable so that it can be used from notebook or other custom Python scripts\n",
    "- be nice to have a GUI for non-technie users\n",
    "- should be easy to explore multiple scenarios\n",
    "- global and scenario specific settings can be managed through settings files, command line args or function args\n",
    "- current occupancy, arrival and departure stats all still desirable\n",
    "- los summary would be nice\n",
    "- outputs should be in formats that lend themselves to further analysis and reporting such as csvs for the occ stats (bydatetime and summary), standard graphic file formats, perhaps JSON for los summary and occ stats\n",
    "- dataset profiling should be done to identify potential issues with horizon effects, warmup effects, missing data periods, or other anomolies.\n",
    "\n",
    "\n",
    "Should hillmaker be redesigned as an OO based application?\n",
    "\n",
    "- does OO design make for a better analyst experience? For example, does OO make it easier to create and manage a bunch of scenarios in which each is a separate hillmaker run? OO would make it easier to document scenarios through their settings (e.g. as json file).\n",
    "- does OO lead to potential performance gains by making it easier to only run the parts we want to run. For example, maybe we don't want individual day of week plots.\n",
    "- right now hillmaker is an (almost) all or nothing experience with each run standing alone. \n",
    "- OO would likely be better for those using hillmaker programmatically. \n",
    "- no matter what the design, there will always be a CLI.\n",
    "- not sure how OO or not affects GUI dev\n",
    "\n",
    "How should hillmaker be redesigned as an OO based application?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other similar projects\n",
    "\n",
    "The [pandas-profiling](https://pandas-profiling.ydata.ai/docs/master/index.html) project has some similarities and has high quality code (certainly better than what I write).\n",
    "\n",
    "- Similar flow of doing analysis on a dataframe and producing various visualizations, reports, and other outputs\n",
    "- Produces plots, html reports, jupyter based report as well as providing results in json format\n",
    "- Uses pydantic to help with config settings management and input validation\n",
    "- Very focused use case - analyze dataframes\n",
    "- Very thorough documentation\n",
    "- The docs on [Changing Settings](https://pandas-profiling.ydata.ai/docs/master/pages/advanced_usage/changing_settings.html) is pretty much what we want to do (except don't need env vars option)\n",
    "- the CLI code is in console.py and it's the `Settings` class that sublclasses Pydantic's `BaseModel` class\n",
    "\n",
    "\n",
    "The [pyfolio](https://github.com/quantopian/pyfolio) project is also good for ideas.\n",
    "\n",
    "- financial analysis of a range of dates for a single stock - see tutorial at https://quantopian.github.io/pyfolio/notebooks/single_stock_example/\n",
    "- other more elaborate analyses\n",
    "- uses a `plotting.context` decorator function to allow plot customization. Matplotlib and seaborn support context managers for temporary changes to plot settings. The matplotlib context handles all the plot details whereas the Seaborn context manager is for higher level changes like plot scaling for different output targets such as notebook, paper or poster.\n",
    "\n",
    "An apache sniffer tool called [thrift](https://github.com/pinterest/thrift-tools)\n",
    "\n",
    "- simple, clean interface\n",
    "- CLI or library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 1 - overall and by patient type summaries\n",
    "\n",
    "Patients flow through a short stay unit for a variety of procedures, tests or therapies. Let's assume patients can be classified into one of five categories of patient types: ART (arterialgram), CAT (post cardiac-cath), MYE (myelogram), IVT (IV therapy), and OTH (other). From one of our hospital information systems we were able to get raw data about the entry and exit times of each patient and exported the data to a csv file. We call each row of such data a *stop* (as in, the patient stopped here for a while). \n",
    "\n",
    "- We want to generate summaries of occupancy as well as arrivals and discharges to go into a summary report for hospital administration. \n",
    "- We want these overall and by patient type. \n",
    "- We also want LOS summaries by patient type. \n",
    "- Volume and occupancy trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter\n",
    "from IPython.display import Image\n",
    "\n",
    "from datetime import datetime, date\n",
    "from typing import Dict, List, Optional, Tuple, Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59877 entries, 0 to 59876\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   PatID      59877 non-null  int64         \n",
      " 1   InRoomTS   59877 non-null  datetime64[ns]\n",
      " 2   OutRoomTS  59877 non-null  datetime64[ns]\n",
      " 3   PatType    59877 non-null  object        \n",
      "dtypes: datetime64[ns](2), int64(1), object(1)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "ssu_stopdata = '../data/ShortStay.csv'\n",
    "stops_df = pd.read_csv(ssu_stopdata, parse_dates=['InRoomTS','OutRoomTS'])\n",
    "stops_df.info() # Check out the structure of the resulting DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatID</th>\n",
       "      <th>InRoomTS</th>\n",
       "      <th>OutRoomTS</th>\n",
       "      <th>PatType</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1996-01-01 07:44:00</td>\n",
       "      <td>1996-01-01 08:50:00</td>\n",
       "      <td>IVT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1996-01-01 08:28:00</td>\n",
       "      <td>1996-01-01 09:20:00</td>\n",
       "      <td>IVT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1996-01-01 11:44:00</td>\n",
       "      <td>1996-01-01 13:30:00</td>\n",
       "      <td>MYE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1996-01-01 11:51:00</td>\n",
       "      <td>1996-01-01 12:55:00</td>\n",
       "      <td>CAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1996-01-01 12:10:00</td>\n",
       "      <td>1996-01-01 13:00:00</td>\n",
       "      <td>IVT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatID            InRoomTS           OutRoomTS PatType\n",
       "0      1 1996-01-01 07:44:00 1996-01-01 08:50:00     IVT\n",
       "1      2 1996-01-01 08:28:00 1996-01-01 09:20:00     IVT\n",
       "2      3 1996-01-01 11:44:00 1996-01-01 13:30:00     MYE\n",
       "3      4 1996-01-01 11:51:00 1996-01-01 12:55:00     CAT\n",
       "4      5 1996-01-01 12:10:00 1996-01-01 13:00:00     IVT"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stops_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new hills scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hillmaker as hm\n",
    "from hillmaker import hmlib as hmlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required inputs\n",
    "scenario_name = 'ss_example_1'\n",
    "in_field_name = 'InRoomTS'\n",
    "out_field_name = 'OutRoomTS'\n",
    "start_date = '1996-01-01'\n",
    "end_date = pd.Timestamp('9/30/1996')\n",
    "\n",
    "# Optional inputs\n",
    "\n",
    "cat_field_name = 'PatType'\n",
    "verbosity = 1 # INFO level logging\n",
    "output_path = './output'\n",
    "bin_size_minutes = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmlib.bin_of_analysis_range(np.datetime64('1996-01-01 12:30'), np.datetime64(start_date), bin_size_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1 = hm.Scenario(scenario_name=scenario_name, \n",
    "                         stops_df=stops_df,\n",
    "                      in_field=in_field_name,\n",
    "                      out_field=out_field_name,\n",
    "                      start_analysis_dt=start_date,\n",
    "                      end_analysis_dt=end_date,\n",
    "                      cat_field=cat_field_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need a pretty print method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bin_size_minutes': 60,\n",
      " 'cap': None,\n",
      " 'cat_field': 'PatType',\n",
      " 'cats_to_exclude': None,\n",
      " 'edge_bins': <EdgeBinsEnum.FRACTIONAL: 1>,\n",
      " 'end_analysis_dt': datetime.date(1996, 9, 30),\n",
      " 'export_bydatetime_csv': True,\n",
      " 'export_dow_plot': True,\n",
      " 'export_summaries_csv': True,\n",
      " 'export_week_plot': True,\n",
      " 'hills': None,\n",
      " 'in_field': 'InRoomTS',\n",
      " 'make_dow_plot': True,\n",
      " 'make_week_plot': True,\n",
      " 'nonstationary_stats': True,\n",
      " 'occ_weight_field': None,\n",
      " 'out_field': 'OutRoomTS',\n",
      " 'output_path': PosixPath('.'),\n",
      " 'percentiles': (0.25, 0.5, 0.75, 0.95, 0.99),\n",
      " 'scenario_name': 'ss_example_1',\n",
      " 'start_analysis_dt': datetime.date(1996, 1, 1),\n",
      " 'stationary_stats': True,\n",
      " 'stops_df':        PatID            InRoomTS           OutRoomTS PatType\n",
      "0          1 1996-01-01 07:44:00 1996-01-01 08:50:00     IVT\n",
      "1          2 1996-01-01 08:28:00 1996-01-01 09:20:00     IVT\n",
      "2          3 1996-01-01 11:44:00 1996-01-01 13:30:00     MYE\n",
      "3          4 1996-01-01 11:51:00 1996-01-01 12:55:00     CAT\n",
      "4          5 1996-01-01 12:10:00 1996-01-01 13:00:00     IVT\n",
      "...      ...                 ...                 ...     ...\n",
      "59872  59873 1996-09-30 19:31:00 1996-09-30 20:15:00     IVT\n",
      "59873  59874 1996-09-30 20:23:00 1996-09-30 21:30:00     IVT\n",
      "59874  59875 1996-09-30 21:00:00 1996-09-30 22:45:00     CAT\n",
      "59875  59876 1996-09-30 21:57:00 1996-09-30 22:40:00     IVT\n",
      "59876  59877 1996-09-30 22:45:00 1996-09-30 23:35:00     CAT\n",
      "\n",
      "[59877 rows x 4 columns],\n",
      " 'verbosity': <VerbosityEnum.WARNING: 0>,\n",
      " 'xlabel': 'Hour',\n",
      " 'ylabel': 'Patients'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_133558/4280743258.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.1.1/migration/\n",
      "  pprint(scenario_1.dict())\n"
     ]
    }
   ],
   "source": [
    "pprint(scenario_1.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario(scenario_name='ss_example_1', stops_df=       PatID            InRoomTS           OutRoomTS PatType\n",
      "0          1 1996-01-01 07:44:00 1996-01-01 08:50:00     IVT\n",
      "1          2 1996-01-01 08:28:00 1996-01-01 09:20:00     IVT\n",
      "2          3 1996-01-01 11:44:00 1996-01-01 13:30:00     MYE\n",
      "3          4 1996-01-01 11:51:00 1996-01-01 12:55:00     CAT\n",
      "4          5 1996-01-01 12:10:00 1996-01-01 13:00:00     IVT\n",
      "...      ...                 ...                 ...     ...\n",
      "59872  59873 1996-09-30 19:31:00 1996-09-30 20:15:00     IVT\n",
      "59873  59874 1996-09-30 20:23:00 1996-09-30 21:30:00     IVT\n",
      "59874  59875 1996-09-30 21:00:00 1996-09-30 22:45:00     CAT\n",
      "59875  59876 1996-09-30 21:57:00 1996-09-30 22:40:00     IVT\n",
      "59876  59877 1996-09-30 22:45:00 1996-09-30 23:35:00     CAT\n",
      "\n",
      "[59877 rows x 4 columns], in_field='InRoomTS', out_field='OutRoomTS', start_analysis_dt=datetime.date(1996, 1, 1), end_analysis_dt=datetime.date(1996, 9, 30), cat_field='PatType', bin_size_minutes=60, cats_to_exclude=None, occ_weight_field=None, percentiles=(0.25, 0.5, 0.75, 0.95, 0.99), nonstationary_stats=True, stationary_stats=True, edge_bins=<EdgeBinsEnum.FRACTIONAL: 1>, output_path=PosixPath('.'), export_bydatetime_csv=True, export_summaries_csv=True, make_dow_plot=True, make_week_plot=True, export_dow_plot=True, export_week_plot=True, cap=None, xlabel='Hour', ylabel='Patients', verbosity=<VerbosityEnum.WARNING: 0>, hills=None)\n"
     ]
    }
   ],
   "source": [
    "pprint(scenario_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scenario_1.bin_size_minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate hills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of [DatetimeIndex(['NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT',\\n               'NaT',\\n               ...\\n               'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT',\\n               'NaT'],\\n              dtype='datetime64[ns]', length=59877, freq=None)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mscenario_1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_hills\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/projects/hillmaker/src/hillmaker/scenario.py:168\u001b[0m, in \u001b[0;36mScenario.make_hills\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_hills\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    167\u001b[0m     inputs_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dump()\n\u001b[0;32m--> 168\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhills \u001b[38;5;241m=\u001b[39m \u001b[43mhm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_hills\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/projects/hillmaker/src/hillmaker/hills.py:176\u001b[0m, in \u001b[0;36mmake_hills\u001b[0;34m(scenario_name, stops_df, in_field, out_field, start_analysis_dt, end_analysis_dt, cat_field, bin_size_minutes, percentiles, cats_to_exclude, occ_weight_field, cap, nonstationary_stats, stationary_stats, export_bydatetime_csv, export_summaries_csv, make_dow_plot, make_week_plot, export_dow_plot, export_week_plot, xlabel, ylabel, output_path, edge_bins, verbosity)\u001b[0m\n\u001b[1;32m    172\u001b[0m     stops_working_df[cat_field] \u001b[38;5;241m=\u001b[39m stops_df[cat_field]\n\u001b[1;32m    174\u001b[0m \u001b[38;5;66;03m# Filter out records that don't overlap the analysis span or have missing entry and/or exit timestamps\u001b[39;00m\n\u001b[1;32m    175\u001b[0m stops_working_df \u001b[38;5;241m=\u001b[39m stops_working_df\u001b[38;5;241m.\u001b[39mloc[(stops_working_df[in_field] \u001b[38;5;241m<\u001b[39m end_analysis_dt_ts) \u001b[38;5;241m&\u001b[39m\n\u001b[0;32m--> 176\u001b[0m                                         (\u001b[38;5;241m~\u001b[39m\u001b[43mstops_working_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstops_working_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43min_field\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39misna()) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m    177\u001b[0m                                         (\u001b[38;5;241m~\u001b[39mstops_working_df[stops_working_df[out_field]]\u001b[38;5;241m.\u001b[39misna()) \u001b[38;5;241m&\u001b[39m\n\u001b[1;32m    178\u001b[0m                                         (stops_working_df[out_field] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m start_analysis_dt_ts)]\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# reset index of df to ensure sequential numbering\u001b[39;00m\n\u001b[1;32m    181\u001b[0m stops_working_df \u001b[38;5;241m=\u001b[39m stops_working_df\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/hm_oo/lib/python3.10/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/hm_oo/lib/python3.10/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/hm_oo/lib/python3.10/site-packages/pandas/core/indexes/base.py:6130\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6129\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of [DatetimeIndex(['NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT',\\n               'NaT',\\n               ...\\n               'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT', 'NaT',\\n               'NaT'],\\n              dtype='datetime64[ns]', length=59877, freq=None)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "scenario_1.make_hills()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.hills.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(scenario_1.hills['plots'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we'd want to be able to quickly view a specific plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_plot(scenario_obj, flow_metric, day_of_week='week'):\n",
    "    flow_metric_str = flow_metric # Possibly allow abbreviations as inputs and then convert to full metric name\n",
    "    day_of_week_str = day_of_week # Again, need to decide on API to make easy on user\n",
    "    plot_name = f'{scenario_obj.scenario_params.scenario_name}_{flow_metric_str}_plot_{day_of_week_str}'\n",
    "    return scenario_obj.hills['plots'][plot_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(scenario_1, 'occupancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(scenario_1, 'occupancy', 'Wed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I added `get_plot()` as a method to the `Scenario` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.get_plot('occupancy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.get_plot('occ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.get_plot('departures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.hills['plots']['s202307251605_arrivals_plot_Tue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.hills['summaries'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.hills['summaries']['nonstationary'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.hills['summaries']['nonstationary']['dow_binofday'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_1.hills['summaries']['nonstationary']['dow_binofday']['occupancy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use case 2 - partition patient types into two holding areas\n",
    "The hospital is considering sending some patient types to a new dedicated holding area. We want to be able to generate hillmaker outputs for various subsets of patients going to each of the two units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df['PatType'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_subset = ['IVT', 'ART']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_subset = [type for type in stops_df['PatType'].unique() if type not in a_subset]\n",
    "b_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_holding_area(pat_type):\n",
    "    if pat_type in a_subset:\n",
    "        return 'unitA'\n",
    "    else:\n",
    "        return 'unitB'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df['new_hold_area'] = stops_df['PatType'].map(lambda x: which_holding_area(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_df['los'] = stops_df['OutRoomTS'] - stops_df['InRoomTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario02 = hm.HillsScenario(stops_df = stops_df, scenario_name = 'scenario02',\n",
    "                              in_field = in_field_name, out_field = out_field_name,\n",
    "                              start_analysis_dt = start_date, end_analysis_dt = end_date,\n",
    "                              cat_field = 'new_hold_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario02.make_hills()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario02.hills['plots'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario02.hills['plots']['scenario02_occupancy_plot_week']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of an operational analysis we would like to compute a number of relevant statistics, such as:\n",
    "\n",
    "- mean and 95th percentile of overall SSU occupancy by hour of day and day of week,\n",
    "- similar hourly statistics for patient arrivals and departures,\n",
    "- all of the above but by patient type as well.\n",
    "\n",
    "In addition to tabular summaries, plots are needed. Like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/ssu-occ.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hillmaker was designed for precisely this type of problem. In fact, the very first version of hillmaker was written for analyzing an SSU when the author was an undergraduate interning at a large health care system. That very first version was written in BASIC on a [DECwriter](https://en.wikipedia.org/wiki/DECwriter)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/DECwriter,_Tektronix,_PDP-11_(192826605).jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\">\n",
    "<font size=\"-2\">Source: By Wolfgang Stief from Tittmoning, Germany - DECwriter, Tektronix, PDP-11, CC0, https://commons.wikimedia.org/w/index.php?curid=105322423</font>\n",
    "</p>\n",
    "\n",
    "Over the years, hillmaker was migrated to [FoxPro](https://en.wikipedia.org/wiki/FoxPro), and then to MS Access where it [lived for many years](http://hillmaker.sourceforge.net/). In 2016, I [moved it to Python](https://misken.github.io/blog/hillmaker-python-released/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hm_oo]",
   "language": "python",
   "name": "conda-env-hm_oo-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
