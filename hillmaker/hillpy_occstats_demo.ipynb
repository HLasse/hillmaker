{
 "metadata": {
  "name": "hillpy_occstats_demo.ipynb"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Computing occupancy statistics with Python - Part 2 of 3#\n",
      "In the second part of this series, we will use Python to compute summary occupancy statistics (such as means and percentiles) by time of day, day of week, and patient category (recall that this example is from a hospital short stay unit - go back to Part 1 for all of the background info). Computation of percentiles by one or more grouping fields is a pain using tools like Excel, Access and SQL Server. With Python+pandas it's easy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Preliminaries##"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the end of Part 1 of this tutorial series, we ended up with a csv file called bydate_shortstay_csv.csv. Let's read it in and take a look at it."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "## Read sample data set and convert string dates to datetimes\n",
      "bydate_df = pd.read_csv('data/bydate_shortstay_csv.csv',parse_dates=['datetime'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bydate_df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<pre>\n",
        "&ltclass 'pandas.core.frame.DataFrame'&gt\n",
        "Int64Index: 5 entries, 0 to 4\n",
        "Data columns (total 8 columns):\n",
        "category      5  non-null values\n",
        "datetime      5  non-null values\n",
        "arrivals      5  non-null values\n",
        "binofday      5  non-null values\n",
        "binofweek     5  non-null values\n",
        "dayofweek     5  non-null values\n",
        "departures    5  non-null values\n",
        "occupancy     5  non-null values\n",
        "dtypes: datetime64[ns](1), float64(6), object(1)\n",
        "</pre>"
       ],
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "  category            datetime  arrivals  binofday  binofweek  dayofweek  \\\n",
        "0      IVT 1996-01-02 00:00:00         0         0         48          1   \n",
        "1      IVT 1996-01-02 00:30:00         0         1         49          1   \n",
        "2      IVT 1996-01-02 01:00:00         0         2         50          1   \n",
        "3      IVT 1996-01-02 01:30:00         0         3         51          1   \n",
        "4      IVT 1996-01-02 02:00:00         0         4         52          1   \n",
        "\n",
        "   departures  occupancy  \n",
        "0           0          0  \n",
        "1           0          0  \n",
        "2           0          0  \n",
        "3           0          0  \n",
        "4           0          0  "
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bydate_df[1320:1350].values"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "array([['IVT', datetime.datetime(1996, 1, 29, 12, 0), 9.0, 24.0, 24.0, 0.0,\n",
        "        9.0, 21.2666666667],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 12, 30), 9.0, 25.0, 25.0,\n",
        "        0.0, 6.0, 22.3333333333],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 13, 0), 12.0, 26.0, 26.0,\n",
        "        0.0, 12.0, 22.2666666667],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 13, 30), 8.0, 27.0, 27.0,\n",
        "        0.0, 9.0, 23.1],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 14, 0), 8.0, 28.0, 28.0, 0.0,\n",
        "        6.0, 22.9333333333],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 14, 30), 9.0, 29.0, 29.0,\n",
        "        0.0, 11.0, 22.3],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 15, 0), 7.0, 30.0, 30.0, 0.0,\n",
        "        6.0, 23.9],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 15, 30), 6.0, 31.0, 31.0,\n",
        "        0.0, 9.0, 22.6],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 16, 0), 9.0, 32.0, 32.0, 0.0,\n",
        "        11.0, 19.7],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 16, 30), 7.0, 33.0, 33.0,\n",
        "        0.0, 4.0, 19.5333333333],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 17, 0), 5.0, 34.0, 34.0, 0.0,\n",
        "        9.0, 18.0333333333],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 17, 30), 2.0, 35.0, 35.0,\n",
        "        0.0, 9.0, 13.6],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 18, 0), 3.0, 36.0, 36.0, 0.0,\n",
        "        5.0, 9.566666666669999],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 18, 30), 2.0, 37.0, 37.0,\n",
        "        0.0, 4.0, 7.166666666669999],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 19, 0), 1.0, 38.0, 38.0, 0.0,\n",
        "        3.0, 5.833333333330001],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 19, 30), 3.0, 39.0, 39.0,\n",
        "        0.0, 2.0, 6.36666666667],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 20, 0), 0.0, 40.0, 40.0, 0.0,\n",
        "        3.0, 4.83333333333],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 20, 30), 0.0, 41.0, 41.0,\n",
        "        0.0, 3.0, 1.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 21, 0), 0.0, 42.0, 42.0, 0.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 21, 30), 0.0, 43.0, 43.0,\n",
        "        0.0, 0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 22, 0), 0.0, 44.0, 44.0, 0.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 22, 30), 0.0, 45.0, 45.0,\n",
        "        0.0, 0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 23, 0), 0.0, 46.0, 46.0, 0.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 29, 23, 30), 0.0, 47.0, 47.0,\n",
        "        0.0, 0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 30, 0, 0), 0.0, 0.0, 48.0, 1.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 30, 0, 30), 0.0, 1.0, 49.0, 1.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 30, 1, 0), 0.0, 2.0, 50.0, 1.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 30, 1, 30), 0.0, 3.0, 51.0, 1.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 30, 2, 0), 0.0, 4.0, 52.0, 1.0,\n",
        "        0.0, 0.0],\n",
        "       ['IVT', datetime.datetime(1996, 1, 30, 2, 30), 0.0, 5.0, 53.0, 1.0,\n",
        "        0.0, 0.0]], dtype=object)"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "With this data frame we can compute all kinds of interesting summary statistics by category, by day of week and time of day. To facilitate this type of \"group by\" analysis, **pandas** takes what is known as the Split-Apply-Combine approach. The [pandas documentation has a nice discussion](http://pandas.pydata.org/pandas-docs/dev/groupby.html) of this. To really understand split-apply-combine, [check out the article](http://www.jstatsoft.org/v40/i01) by [Hadley Wickham](http://had.co.nz/) who created the **plyr** package for [R](http://www.r-project.org/). I also created a tutorial on [Getting started with Python (with pandas and matplotlib) for group by analysis](http://hselab.org/machinery/content/getting-started-python-pandas-and-matplotlib-group-analysis) that covers some of the basics. A [companion tutorial shows how to do the same analysis using R](http://hselab.org/machinery/content/getting-started-r-plyr-and-ggplot2-group-analysis) instead of Python."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Pandas provides a `GroupBy` object to facilitate computing aggregate statistics by grouping fields. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create a GroupBy object for the summary stats    \n",
      "bydate_dfgrp1 = bydate_df.groupby(['category','binofweek'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Having a group by object makes it easy to compute statistics such as the mean of all of the fields other than the grouping fields.\n",
      "# You'll see that the result is simply another DataFrame.\n",
      "bydate_dfgrp1.mean()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<pre>\n",
        "&ltclass 'pandas.core.frame.DataFrame'&gt\n",
        "MultiIndex: 2016 entries, (ART, 0.0) to (Total, 335.0)\n",
        "Data columns (total 5 columns):\n",
        "arrivals      2016  non-null values\n",
        "binofday      2016  non-null values\n",
        "dayofweek     2016  non-null values\n",
        "departures    2016  non-null values\n",
        "occupancy     2016  non-null values\n",
        "dtypes: float64(5)\n",
        "</pre>"
       ],
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "MultiIndex: 2016 entries, (ART, 0.0) to (Total, 335.0)\n",
        "Data columns (total 5 columns):\n",
        "arrivals      2016  non-null values\n",
        "binofday      2016  non-null values\n",
        "dayofweek     2016  non-null values\n",
        "departures    2016  non-null values\n",
        "occupancy     2016  non-null values\n",
        "dtypes: float64(5)"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Let's explore some of the means.\n",
      "bydate_dfgrp1.mean()[100:120]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th>arrivals</th>\n",
        "      <th>binofday</th>\n",
        "      <th>dayofweek</th>\n",
        "      <th>departures</th>\n",
        "      <th>occupancy</th>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>category</th>\n",
        "      <th>binofweek</th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "      <th></th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th rowspan=\"20\" valign=\"top\">ART</th>\n",
        "      <th>100</th>\n",
        "      <td> 0.000000</td>\n",
        "      <td>  4</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>101</th>\n",
        "      <td> 0.000000</td>\n",
        "      <td>  5</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>102</th>\n",
        "      <td> 0.000000</td>\n",
        "      <td>  6</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>103</th>\n",
        "      <td> 0.000000</td>\n",
        "      <td>  7</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>104</th>\n",
        "      <td> 0.000000</td>\n",
        "      <td>  8</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>105</th>\n",
        "      <td> 0.000000</td>\n",
        "      <td>  9</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>106</th>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 10</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.000000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>107</th>\n",
        "      <td> 1.538462</td>\n",
        "      <td> 11</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 0.782051</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>108</th>\n",
        "      <td> 1.769231</td>\n",
        "      <td> 12</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 2.361538</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>109</th>\n",
        "      <td> 3.384615</td>\n",
        "      <td> 13</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 5.058974</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>110</th>\n",
        "      <td> 1.769231</td>\n",
        "      <td> 14</td>\n",
        "      <td> 2</td>\n",
        "      <td> 0.000000</td>\n",
        "      <td> 7.674359</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>111</th>\n",
        "      <td> 1.538462</td>\n",
        "      <td> 15</td>\n",
        "      <td> 2</td>\n",
        "      <td> 3.076923</td>\n",
        "      <td> 7.584615</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>112</th>\n",
        "      <td> 1.692308</td>\n",
        "      <td> 16</td>\n",
        "      <td> 2</td>\n",
        "      <td> 3.384615</td>\n",
        "      <td> 5.225641</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>113</th>\n",
        "      <td> 1.692308</td>\n",
        "      <td> 17</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1.538462</td>\n",
        "      <td> 5.300000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>114</th>\n",
        "      <td> 2.153846</td>\n",
        "      <td> 18</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1.923077</td>\n",
        "      <td> 5.282051</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>115</th>\n",
        "      <td> 1.846154</td>\n",
        "      <td> 19</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1.923077</td>\n",
        "      <td> 5.412821</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>116</th>\n",
        "      <td> 1.153846</td>\n",
        "      <td> 20</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1.923077</td>\n",
        "      <td> 5.228205</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>117</th>\n",
        "      <td> 1.461538</td>\n",
        "      <td> 21</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1.230769</td>\n",
        "      <td> 4.800000</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>118</th>\n",
        "      <td> 1.692308</td>\n",
        "      <td> 22</td>\n",
        "      <td> 2</td>\n",
        "      <td> 2.000000</td>\n",
        "      <td> 4.764103</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>119</th>\n",
        "      <td> 2.153846</td>\n",
        "      <td> 23</td>\n",
        "      <td> 2</td>\n",
        "      <td> 1.461538</td>\n",
        "      <td> 5.064103</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "                    arrivals  binofday  dayofweek  departures  occupancy\n",
        "category binofweek                                                      \n",
        "ART      100        0.000000         4          2    0.000000   0.000000\n",
        "         101        0.000000         5          2    0.000000   0.000000\n",
        "         102        0.000000         6          2    0.000000   0.000000\n",
        "         103        0.000000         7          2    0.000000   0.000000\n",
        "         104        0.000000         8          2    0.000000   0.000000\n",
        "         105        0.000000         9          2    0.000000   0.000000\n",
        "         106        0.000000        10          2    0.000000   0.000000\n",
        "         107        1.538462        11          2    0.000000   0.782051\n",
        "         108        1.769231        12          2    0.000000   2.361538\n",
        "         109        3.384615        13          2    0.000000   5.058974\n",
        "         110        1.769231        14          2    0.000000   7.674359\n",
        "         111        1.538462        15          2    3.076923   7.584615\n",
        "         112        1.692308        16          2    3.384615   5.225641\n",
        "         113        1.692308        17          2    1.538462   5.300000\n",
        "         114        2.153846        18          2    1.923077   5.282051\n",
        "         115        1.846154        19          2    1.923077   5.412821\n",
        "         116        1.153846        20          2    1.923077   5.228205\n",
        "         117        1.461538        21          2    1.230769   4.800000\n",
        "         118        1.692308        22          2    2.000000   4.764103\n",
        "         119        2.153846        23          2    1.461538   5.064103"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've seen how the a `GroupBy` object works, let's see how we can compute a whole bunch of summary statistics at once. Specifically we want to compute the mean, standard deviation, min, max and several percentiles. First let's create a slightly different `GroupBy` object."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bydate_dfgrp2 = bydate_df.groupby(['category','dayofweek','binofday'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now let's define a function that will return a bunch of statistics in a dictionary for a column of data."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_occstats(group, stub=''):\n",
      "    return {stub+'count': group.count(), stub+'mean': group.mean(), \n",
      "            stub+'min': group.min(),\n",
      "            stub+'max': group.max(), 'stdev': group.std(), \n",
      "            stub+'p50': group.quantile(0.5), stub+'p55': group.quantile(0.55),\n",
      "            stub+'p60': group.quantile(0.6), stub+'p65': group.quantile(0.65),\n",
      "            stub+'p70': group.quantile(0.7), stub+'p75': group.quantile(0.75),\n",
      "            stub+'p80': group.quantile(0.8), stub+'p85': group.quantile(0.85),\n",
      "            stub+'p90': group.quantile(0.9), stub+'p95': group.quantile(0.95),\n",
      "            stub+'p975': group.quantile(0.975), \n",
      "            stub+'p99': group.quantile(0.99)}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can use the `apply` function to apply the `get_occstats()` function to a data series. We'll create separate output data series for occupancy, arrivals and departures.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "occ_stats = bydate_dfgrp2['occupancy'].apply(get_occstats)\n",
      "arr_stats = bydate_dfgrp2['arrivals'].apply(get_occstats)\n",
      "dep_stats = bydate_dfgrp2['departures'].apply(get_occstats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, what is `occ_stats`?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "type(occ_stats)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "pandas.core.series.Series"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It's a pandas `Series` object. What does its index look like?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "occ_stats.index"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 13,
       "text": [
        "MultiIndex\n",
        "[ART  0  0  count,            max  ,            mean ,            min  ,            p50  ,            p55  ,            p60  ,            p65  ,            p70  ,            p75  ,            p80  ,            p85  ,            p90  ,            p95  ,            p975 ,            p99  ,            stdev,         1  count,            max  ,            mean ,            min  ,            p50  ,            p55  ,            p60  ,            p65  ,            p70  ,            p75  ,            p80  ,            p85  ,            p90  ,            p95  ,            p975 ,            p99  ,            stdev,         2  count,            max  ,            mean ,            min  ,            p50  ,            p55  ,            p60  ,            p65  ,            p70  ,            p75  ,            p80  ,            p85  ,            p90  ,            p95  ,            p975 ,            p99  , ..., Total  6  45  max  ,               mean ,               min  ,               p50  ,               p55  ,               p60  ,               p65  ,               p70  ,               p75  ,               p80  ,               p85  ,               p90  ,               p95  ,               p975 ,               p99  ,               stdev,           46  count,               max  ,               mean ,               min  ,               p50  ,               p55  ,               p60  ,               p65  ,               p70  ,               p75  ,               p80  ,               p85  ,               p90  ,               p95  ,               p975 ,               p99  ,               stdev,           47  count,               max  ,               mean ,               min  ,               p50  ,               p55  ,               p60  ,               p65  ,               p70  ,               p75  ,               p80  ,               p85  ,               p90  ,               p95  ,               p975 ,               p99  ,               stdev]"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice it's a `MultiIndex` with 4 levels: category, dayofweek, binofday, statistic. It would be nice to \"un-pivot\" the statistic from the index and have it correspond to a set of columns. That's what `unstack()` will do. It will leave us with a `DataFrame` with all of the statistics as columns and a 3 level multi-index of category, dayofweek and binofday. Perfect for plotting."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "occ_stats.unstack()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<pre>\n",
        "&ltclass 'pandas.core.frame.DataFrame'&gt\n",
        "MultiIndex: 2016 entries, (ART, 0.0, 0.0) to (Total, 6.0, 47.0)\n",
        "Data columns (total 17 columns):\n",
        "count    2016  non-null values\n",
        "max      2016  non-null values\n",
        "mean     2016  non-null values\n",
        "min      2016  non-null values\n",
        "p50      2016  non-null values\n",
        "p55      2016  non-null values\n",
        "p60      2016  non-null values\n",
        "p65      2016  non-null values\n",
        "p70      2016  non-null values\n",
        "p75      2016  non-null values\n",
        "p80      2016  non-null values\n",
        "p85      2016  non-null values\n",
        "p90      2016  non-null values\n",
        "p95      2016  non-null values\n",
        "p975     2016  non-null values\n",
        "p99      2016  non-null values\n",
        "stdev    2016  non-null values\n",
        "dtypes: float64(17)\n",
        "</pre>"
       ],
       "output_type": "pyout",
       "prompt_number": 14,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "MultiIndex: 2016 entries, (ART, 0.0, 0.0) to (Total, 6.0, 47.0)\n",
        "Data columns (total 17 columns):\n",
        "count    2016  non-null values\n",
        "max      2016  non-null values\n",
        "mean     2016  non-null values\n",
        "min      2016  non-null values\n",
        "p50      2016  non-null values\n",
        "p55      2016  non-null values\n",
        "p60      2016  non-null values\n",
        "p65      2016  non-null values\n",
        "p70      2016  non-null values\n",
        "p75      2016  non-null values\n",
        "p80      2016  non-null values\n",
        "p85      2016  non-null values\n",
        "p90      2016  non-null values\n",
        "p95      2016  non-null values\n",
        "p975     2016  non-null values\n",
        "p99      2016  non-null values\n",
        "stdev    2016  non-null values\n",
        "dtypes: float64(17)"
       ]
      }
     ],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "occ_stats_summary = occ_stats.unstack()\n",
      "arr_stats_summary = arr_stats.unstack()\n",
      "dep_stats_summary = dep_stats.unstack()\n",
      "\n",
      "print occ_stats_summary[200:220].values # Let's peek into the middle of the table."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[  1.30000000e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00]\n",
        " [  1.30000000e+01   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00]\n",
        " [  1.30000000e+01   1.33333333e-01   1.02564103e-02   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
        "    0.00000000e+00   5.33333333e-02   9.33333333e-02   1.17333333e-01\n",
        "    3.69800131e-02]\n",
        " [  1.30000000e+01   1.60000000e+00   3.25641026e-01   0.00000000e+00\n",
        "    0.00000000e+00   6.00000000e-02   1.80000000e-01   4.20000000e-01\n",
        "    5.53333333e-01   6.33333333e-01   6.53333333e-01   6.80000000e-01\n",
        "    7.20000000e-01   1.08000000e+00   1.34000000e+00   1.49600000e+00\n",
        "    4.85179204e-01]\n",
        " [  1.30000000e+01   4.50000000e+00   1.80000000e+00   0.00000000e+00\n",
        "    1.96666667e+00   2.02666667e+00   2.14666667e+00   2.38666667e+00\n",
        "    2.49333333e+00   2.53333333e+00   2.71333333e+00   2.88666667e+00\n",
        "    3.04666667e+00   3.66000000e+00   4.08000000e+00   4.33200000e+00\n",
        "    1.29092272e+00]\n",
        " [  1.30000000e+01   7.96666667e+00   5.14358974e+00   2.26666667e+00\n",
        "    4.93333333e+00   5.07333333e+00   5.22666667e+00   5.40666667e+00\n",
        "    5.84000000e+00   6.40000000e+00   6.48000000e+00   6.74000000e+00\n",
        "    7.36000000e+00   7.72666667e+00   7.84666667e+00   7.91866667e+00\n",
        "    1.61900659e+00]\n",
        " [  1.30000000e+01   1.11666667e+01   8.49743590e+00   6.26666667e+00\n",
        "    7.96666667e+00   8.36666667e+00   8.73333333e+00   9.03333333e+00\n",
        "    9.22666667e+00   9.36666667e+00   9.46666667e+00   9.70000000e+00\n",
        "    1.02000000e+01   1.06866667e+01   1.09266667e+01   1.10706667e+01\n",
        "    1.36866125e+00]\n",
        " [  1.30000000e+01   1.07333333e+01   8.70769231e+00   5.16666667e+00\n",
        "    9.20000000e+00   9.48000000e+00   9.66666667e+00   9.66666667e+00\n",
        "    9.68000000e+00   9.70000000e+00   9.86000000e+00   1.00400000e+01\n",
        "    1.02600000e+01   1.04933333e+01   1.06133333e+01   1.06853333e+01\n",
        "    1.57298689e+00]\n",
        " [  1.30000000e+01   8.66666667e+00   6.04615385e+00   2.83333333e+00\n",
        "    6.53333333e+00   6.61333333e+00   6.68000000e+00   6.72000000e+00\n",
        "    6.88000000e+00   7.10000000e+00   7.48000000e+00   7.86666667e+00\n",
        "    8.26666667e+00   8.50666667e+00   8.58666667e+00   8.63466667e+00\n",
        "    1.83381501e+00]\n",
        " [  1.30000000e+01   9.06666667e+00   6.27948718e+00   3.50000000e+00\n",
        "    6.33333333e+00   6.35333333e+00   6.48000000e+00   6.82000000e+00\n",
        "    6.96000000e+00   7.00000000e+00   7.06000000e+00   7.47333333e+00\n",
        "    8.59333333e+00   9.00666667e+00   9.03666667e+00   9.05466667e+00\n",
        "    1.58707385e+00]\n",
        " [  1.30000000e+01   8.80000000e+00   6.31025641e+00   4.00000000e+00\n",
        "    6.50000000e+00   6.76000000e+00   6.96666667e+00   7.06666667e+00\n",
        "    7.22000000e+00   7.40000000e+00   7.62000000e+00   7.80000000e+00\n",
        "    7.90000000e+00   8.28000000e+00   8.54000000e+00   8.69600000e+00\n",
        "    1.49172934e+00]\n",
        " [  1.30000000e+01   7.56666667e+00   6.07435897e+00   3.83333333e+00\n",
        "    5.96666667e+00   6.46666667e+00   6.84000000e+00   6.96000000e+00\n",
        "    7.05333333e+00   7.13333333e+00   7.19333333e+00   7.28000000e+00\n",
        "    7.42000000e+00   7.50666667e+00   7.53666667e+00   7.55466667e+00\n",
        "    1.21595518e+00]\n",
        " [  1.30000000e+01   9.50000000e+00   5.78461538e+00   2.53333333e+00\n",
        "    5.53333333e+00   5.87333333e+00   6.15333333e+00   6.31333333e+00\n",
        "    6.47333333e+00   6.63333333e+00   6.75333333e+00   7.01333333e+00\n",
        "    7.55333333e+00   8.44000000e+00   8.97000000e+00   9.28800000e+00\n",
        "    1.73614171e+00]\n",
        " [  1.30000000e+01   9.90000000e+00   6.27179487e+00   3.83333333e+00\n",
        "    5.70000000e+00   5.78000000e+00   5.86000000e+00   5.94000000e+00\n",
        "    6.56666667e+00   7.46666667e+00   7.64666667e+00   7.94666667e+00\n",
        "    8.48666667e+00   9.16000000e+00   9.53000000e+00   9.75200000e+00\n",
        "    1.71830268e+00]\n",
        " [  1.30000000e+01   9.00000000e+00   6.34358974e+00   2.63333333e+00\n",
        "    6.63333333e+00   6.67333333e+00   6.79333333e+00   7.07333333e+00\n",
        "    7.24666667e+00   7.36666667e+00   7.52666667e+00   7.64666667e+00\n",
        "    7.68666667e+00   8.22000000e+00   8.61000000e+00   8.84400000e+00\n",
        "    1.63409192e+00]\n",
        " [  1.30000000e+01   9.10000000e+00   5.69230769e+00   3.53333333e+00\n",
        "    5.10000000e+00   5.44000000e+00   5.80666667e+00   6.22666667e+00\n",
        "    6.66000000e+00   7.10000000e+00   7.30000000e+00   7.44666667e+00\n",
        "    7.48666667e+00   8.14000000e+00   8.62000000e+00   8.90800000e+00\n",
        "    1.69733745e+00]\n",
        " [  1.30000000e+01   8.90000000e+00   5.59230769e+00   2.80000000e+00\n",
        "    5.43333333e+00   5.45333333e+00   5.56666667e+00   5.86666667e+00\n",
        "    6.11333333e+00   6.33333333e+00   6.63333333e+00   6.99333333e+00\n",
        "    7.47333333e+00   8.14000000e+00   8.52000000e+00   8.74800000e+00\n",
        "    1.59392097e+00]\n",
        " [  1.30000000e+01   8.56666667e+00   5.28205128e+00   3.00000000e+00\n",
        "    5.16666667e+00   5.22666667e+00   5.34000000e+00   5.56000000e+00\n",
        "    5.75333333e+00   5.93333333e+00   6.17333333e+00   6.48666667e+00\n",
        "    6.94666667e+00   7.68666667e+00   8.12666667e+00   8.39066667e+00\n",
        "    1.49267442e+00]\n",
        " [  1.30000000e+01   8.33333333e+00   4.77948718e+00   2.30000000e+00\n",
        "    4.80000000e+00   4.92000000e+00   5.10666667e+00   5.42666667e+00\n",
        "    5.53333333e+00   5.53333333e+00   6.01333333e+00   6.40000000e+00\n",
        "    6.60000000e+00   7.33333333e+00   7.83333333e+00   8.13333333e+00\n",
        "    1.76962742e+00]\n",
        " [  1.30000000e+01   8.06666667e+00   4.81794872e+00   2.26666667e+00\n",
        "    4.83333333e+00   4.93333333e+00   5.07333333e+00   5.29333333e+00\n",
        "    5.39333333e+00   5.43333333e+00   5.65333333e+00   5.90666667e+00\n",
        "    6.22666667e+00   7.02666667e+00   7.54666667e+00   7.85866667e+00\n",
        "    1.49329460e+00]]\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wouldn't it be nice if Excel Pivot Tables could produce the output above? Why can't they? Because they can't do things like percentiles (or other custom aggregate functions). I love spreadsheets. I teach spreadsheet modeling. However, I find myself using either Python+pandas+matplotlib or R+plyr+ggplot2 more and more frequently for things I used to do in Excel."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's fire these guys out to csv files so we can check them out and maybe play with them in spreadsheet. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "occ_stats_summary.to_csv('occ_stats_summary.csv')\n",
      "arr_stats_summary.to_csv('arr_stats_summary.csv')\n",
      "dep_stats_summary.to_csv('dep_stats_summary.csv')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The real reason I exported them to csv was to make it easy to read these results back in for Part 3 of this series of tutorials. In Part 3, we'll create some plots using matplotlib based on these summary statistics."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}